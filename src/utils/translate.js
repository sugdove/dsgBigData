
export const translateJsonData = {
  welcome: '欢迎使用本应用',
  hello: '你好',
  HELLO: '你好',
  'get-lang': '获取语言类型',
  DSG: '迪思杰',
  'globalization test': '国际化测试',

  'Feed Manager': '数据流程管理',
  Feeds: '流程工具',
  Categories: '分类工具',
  Tables: '数据查询',
  SLA: '服务水平协议',
  'Visual Query': '可视化分析',

  Operations: '运营监控',
  Dashboard: '全局',
  Services: '服务',
  Jobs: '作业',
  Alerts: '告警',
  Tasks: '调度任务',
  Charts: '图表',
  'Abandon All': '遗弃所有',
  Ops: '监控',

  Admin: '管理',
  'Data Sources': '数据源',
  'Domain Types': '域类型',
  Properties: '属性',
  Templates: '模板',
  Users: '用户',
  Groups: '用户组',
  'SLA Email': '服务水平协议邮件',

  'Services Health': '服务健康状况',
  'Monitors All Service Health': '监控所有服务监控',
  Healthy: '健康',
  HEALTHY: '健康',
  Unhealthy: '不健康',
  UNHEALTHY: '不健康',

  'Feed Health': '数据流程健康状况',
  STREAM: '流数据',
  running: '正在运行',
  RUNNING: '正在运行',
  Status: '状态',
  Since: '自',
  'Last Run Time': '上次运行时间',
  Feed: '流程',
  Health: '监控状况',
  SCHEDULED: '计划',

  'Job Activity': '作业活动',
  'Currently Running Jobs': '当前运行的作业',
  Running: '正在运行',

  'Data Confidence': '数据质量',
  'Validates Data Integrity': '验证数据一致性',
  Alerts: '告警',
  ALERT: '告警',
  ALERTS: '告警',
  'No alerts': '无告警',
  Days: '天数',
  Hours: '小时',
  Minutes: '分钟',
  Seconds: '秒',
  'No later than time': '不得迟于时间',
  FeedName: '流程名称',
  'Expected Delivery Time': '预期交付时间',
  'Act upon a Feed Failure': '在流程失败时采取行动。',

  All: '所有',
  'Less than...': '少于...',
  'Greater than...': '大于...',
  'Equal to...': '等于...',
  'Contains...': '包含...',
  'Lower Case': '小写',
  'Title Case': '首字母大写',
  Trim: '对齐',
  'Upper Case': '大写',
  'Use CTRL + space for autocomplete on table and column names': '在表和列名称上使用CTRL +空格来自动完成',
  'Data Transformation': '数据转换',
  'Customize destination table': '自定义目标表',
  'Users pick and choose different data tables and columns and apply functions to transform the data to their desired destination table.': '用户选择并选择不同的数据表和列，并应用函数将数据转换为所需的目标表。',
  'Allow users to define and customize the destination data table.': '允许用户定义和定制目标数据表。',
  Streaming: '流数据',
  FAILED: '失败',
  COMPLETED: '已完成',
  Completed: '已完成',
  Abandoned: '遗弃',
  STARTED: '已启动',
  INITIAL: '初始',
  ABANDONED: '遗弃',
  UNHANDLED: '未处理',
  PAUSED: '暂停',
  STOPPED: '停止',
  ENABLED: '启用',
  enabled: '启用',
  EXECUTING: '执行',
  DISABLED: '禁用',
  'Full Load': '满载',
  Title: '标题',
  Members: '成员',
  Incremental: '增加',
  Sync: '同步',
  'Rolling sync': '滚动同步',
  'Replace content in matching partitions': '替换匹配分区中的内容',
  Merge: '合并',
  'Insert all rows': '插入所有行',
  'Dedupe and merge': '重复和合并',
  DEDUPE_AND_MERGE: '重复和合并',
  'Insert rows ignoring duplicates': '插入忽略重复行',
  'Merge using primary key': '使用主键合并',
  'Upsert using primary key': '使用主键的Upsert',
  'Replace table content': '替换整个表内容',
  'How should the results be returned.  Either a Delimited output such as CSV, or AVRO.  If delimited you must specify the delimiter.': '应该如何返回结果。 分隔输出，如CSV或AVRO。 如果分隔，则必须指定分隔符。',
  'Replace entire table': '替换整个表格',
  disabled: '禁用',
  'Service Name': '服务名',
  'Update Date': '更新日期',
  'Hide Cleared': '隐藏已清除',
  'Show Cleared': '显示已清除',
  'Incremental load based on a  high watermark': '基于高阀值的增量加载',
  'Component Name': '组件名',
  Failed: '失败',
  'Source Database Connection': '源数据库连接',
  'Load Strategy': '加载策略',
  'Date Field': '日期字段',
  'Output Type': '输出类型',
  Database: '数据库',
  'DB Avro Ingest Parameters': '数据库Avro接收参数',
  'Active Water Mark Strategy': '激活最大阀值标记策略',
  'High-Water Mark': '高阀值',
  Mode: '模式',
  'Indicates whether this processor should commit or reject high-water mark updates': '指示此处理器是否应提交或拒绝高阀值标记更新',
  'Delete Attributes Expression': '删除属性表达式',
  'Release All': '释放全部',
  'Overlap Period': '重叠期',
  'Source Fields': '源字段',
  'Source Table': '源数据表',
  'Initialize Cleanup Parameters': '初始化清理参数',
  category: '类别',
  'Average Duration': '平均持续时间',
  'Bytes In': '字节输入',
  'Bytes Out': '字节输出',
  'Flows Started': '流程启动',
  'Flows Finished': '流程完成',
  'Total Events': '总事件数',
  'Failed Events': '失败事件数',
  'Max Wait Time': '最长等待时间',
  'The maximum amount of time allowed for a running SQL select query  , zero means there is no limit. Max time less than 1 second will be equal to zero.': '运行SQL选择查询所允许的最大时间量为零意味着没有限制。 最大时间小于1秒将等于零。',

  feed: '流程',
  FEED: '流程',
  'Feed Details': '流程详情',
  'Job Execution': '作业执行',
  'Job Failure': '作业失败',
  'job failure': '作业失败',
  'hdfs.ingest.root': 'hdfs采集根部',
  'hive.ingest.root': 'hive采集根部',
  'hive.master.root': 'hive主要根部',
  'hive.profile.root': 'hive配置根部',
  'metadata.table.fieldPoliciesJson': '元数据表字段策略',
  'metadata.table.fieldStructure': '元数据表字段结构',
  'metadata.table.partitionSpecs': '元数据表分区规格',
  'metadata.table.partitionStructure': '元数据表分区结构',
  'metadata.table.targetFormat': '元数据表目标格式',
  'metadata.table.targetMergeStrategy': '元数据表目标合并策略',
  'metadata.table.targetTblProperties': '元数据表目标tbl属性',
  'No Data - Release Highwater Mark': '无数据 - 释放高阀值标记',
  'Update flow parameters': '更新流参数',
  'Amount of time to overlap into the last load date to ensure long running transactions missed by previous load weren\'t missed. Recommended: >0s': '与上次加载日期重叠的时间量以确保长时间运行的业务没有被错过。 推荐：> 0s',
  'Fetch RDBMS Data': '获取RDBMS数据',
  'Minimum Time Unit': '最小时间单位',
  'Output Delimiter': '输出分隔符',
  'Used only if the Output Type is \'Delimited\'.  If this is empty and the Output Type is delimited it will default to a \',\'.  This property is not used if the Output Type is AVRO.': '仅在输出类型为“分隔”时使用。 如果这是空的，并且输出类型被分隔，它将默认为\'，\'。 如果输出类型是AVRO，则不使用此属性。',
  'The minimum unit of data eligible to load. For the ILM case, this would be DAY, WEEK, MONTH, YEAR , zero means there is no limit. Max time less than 1 second will be equal to zero.': '有资格加载的最小单位数据。 对于ILM的情况，这将是DAY，WEEK，MONTH，YEAR，零意味着没有限制。 最大时间小于1秒将等于零。',
  'Backoff Period': '回退期',
  'High-Water Mark Property Name': '高阀值标记属性名称',
  'Name of the flow file attribute that should contain the current hig-water mark date, and which this processor will update with new values.  Required if the load strategy is set to INCREMENTAL.': '应包含当前高阀值标记日期的流文件属性的名称，以及此处理器将使用新值更新的流文件属性的名称。 如果加载策略设置为INCREMENTAL，则是必需的。',
  'Only records older than the backoff period will be eligible for pickup. This can be used in the ILM use case to define a retention period. Recommended: >5m': '只有回退期之前的记录才有资格提取。 这可以在ILM用例中用于定义保留期。 建议：> 5米',
  'If true, commits or rolls back all pending high-water marks.  Otherwise, commits/rolls back only the named water mark property.': '如果属实，则提交或回滚所有悬而未决的高阀值标记。 否则，仅提交/回滚指定的阀值属性。',
  'Stateful Variables Initial Value': '有状态变量初始值',
  'High-Water Mark Value Property Name': '最大阀值标记值属性名称',
  'Initial Value': '初始值',
  'Initialize Feed Parameters': '初始化流程参数',
  'How should the results be returned. Either a Delimited output such as CSV, or AVRO. If delimited you must specify the delimiter.': '应该如何返回结果。 分隔输出，如CSV或AVRO。 如果分隔，则必须指定分隔符。',
  'Max Yield Count': '最大产量数',
  'Metadata Service': '元数据服务',
  'Think Big metadata service': '认为大元数据服务',
  'System feed category': '系统流程类别',
  'System feed name': '系统流程名称',
  Filesystem: '文件系统',
  'File Filter': '文件过滤',
  'Ignore Hidden Files': '忽略隐藏的文件',
  'Input Directory': '输入目录',
  'Keep Source File': '保持源文件',
  'Maximum File Age': '最大文件时间',
  'Maximum File Size': '最大文件大小',
  'Minimum File Size': '最小文件大小',
  'The minimum size that a file must be in order to be pulled': '文件必须被拉的最小尺寸',
  'The maximum size that a file can be in order to be pulled': '文件可以被拉出的最大大小',
  'Path Filter': '路径过滤',
  'Polling Interval': '轮询间隔',
  'Recurse Subdirectories': '递归子目录',
  TriggerCleanup: '触发清理',
  'Run Feed': '运行流程',
  'Batch Size': '批量大小',
  'If true, each FlowFile that is generated will be unique. If false, a random value will be generated and all FlowFiles will get the same content but this offers much higher throughput': '如果为true，则生成的每个FlowFile将是唯一的。 如果为false，则会生成随机值，所有FlowFile将获得相同的内容，但是这会提供更高的吞吐量',
  'The number of FlowFiles to be transferred in each invocation': '每个调用中要传输的FlowFiles的数量',
  'Data Format': '数据格式',
  'character-set': '字符集',
  'Trigger Cleanup': '触发清理',
  'Store State': '存储状况',
  'Select whether or not state will be stored. Selecting \'Stateless\' will offer the default functionality of purely updating the attributes on a FlowFile in a stateless manner. Selecting a stateful option will not only store the attributes on the FlowFile but also in the Processors state. See the \'Stateful Usage\' topic of the \'Additional Details\' section of this processor\'s documentation for more information': '选择是否存储状态。 选择“无状态”将提供以无状态方式纯粹更新FlowFile上属性的默认功能。 选择一个有状态选项不仅会将属性存储在FlowFile中，还会存储在Processors状态中。 有关更多信息，请参阅此处理器文档的“其他详细信息”部分的“状态使用”主题',
  'Regular expression for attributes to be deleted from FlowFiles.  Existing attributes that match will be deleted regardless of whether they are updated by this processor.': '要从FlowFiles中删除的属性的正则表达式。 无论是否由该处理器更新，匹配的现有属性都将被删除。',
  'If using state to set/reference variables then this value is used to set the initial value of the stateful variable. This will only be used in the @OnScheduled method when state does not contain a value for the variable. This is required if running statefully but can be empty if needed.': '如果使用状态来设置/引用变量，则此值用于设置有状态变量的初始值。 只有在状态不包含变量的值时，才会使用@OnScheduled方法。 如果有状态运行，这是必需的，但如果需要可以是空的。',
  'generate-ff-custom-text': '产生-FF-自定义文本',
  'If Data Format is text and if Unique FlowFiles is false, then this custom text will be used as content of the generated FlowFiles and the File Size will be ignored. Finally, if Expression Language is used, evaluation will be performed only once per batch of generated FlowFiles': '如果数据格式是文本，并且如果唯一FlowFiles为false，则此自定义文本将用作生成的FlowFiles的内容，并且文件大小将被忽略。 最后，如果使用表达式语言，每批生成的FlowFiles只进行一次评估',
  'Specifies the character set to use when writing the bytes of Custom Text to a flow file.': '指定将自定义文本的字节写入流文件时要使用的字符集。',
  'Specifies whether the data should be Text or Binary': '指定数据是文本还是二进制',
  'File Size': '文件大小',
  archiveId: '存档',
  'Access Denied.  You are unable to edit the template.': '拒绝访问。 您无法编辑模板。',
  'metadata.table.feedFormat': '元数据表流程格式',
  'metadata.table.fieldIndexString': '元数据表字段索引字符串',
  'hdfs.archive.root': 'hdfs存档根部',
  'The maximum number of yields, if the yield strategy is selected, that should be attempted before failures to obtain a high-water mark are routed to the "active" relationship (routing never occurs if unset or less than zero)': '如果选择了产量策略，那么最大产量数量应该在获得最大阀值标记失败之前尝试发送到“主动”关系（如果未设置或小于零，路由将不会发生）',
  'Unique FlowFiles': '独特的FlowFiles',
  'metadata.table.feedFieldStructure': '元数据表字段结构',
  skipHeader: '跳过标题行',
  'Detect DB Avro Ingest': '检测数据库Avro摄取',
  'Routing Strategy': '路由策略',
  'table-avro': '表的avro',
  'Failed - Release Highwater Mark': '失败 - 释放高阀值标记',
  'Specifies how to determine which relationship to use when evaluating the Expression Language': '指定在评估表达式语言时如何确定使用哪种关系',
  'The size of the file that will be used': '将被使用的文件的大小',
  'Feed Cleanup Event Service': '流程清理事件服务',
  'Metadata Provider Service': '元数据提供者服务',
  'Service supplying the implementations of the various metadata providers': '服务提供各种元数据提供者的实现',
  'Service that manages the cleanup of feeds.': '管理流程清理事件的服务',
  'Indicates whether or not to pull files from subdirectories': '指示是否从子目录中提取文件',
  'Indicates how long to wait before performing a directory listing': '表示执行目录列表之前需要等待多长时间',
  'When Recurse Subdirectories is true, then only subdirectories whose path matches the given regular expression will be scanned': '递归子目录为true时，只扫描路径与给定正则表达式匹配的子目录',
  'Minimum File Age': '文件最小时间',
  'The minimum age that a file must be in order to be pulled; any file younger than this amount of time (according to last modification date) will be ignored': '文件必须被拖动的最小时间; 任何小于此时间（根据上次修改日期）的文件都将被忽略',
  'The maximum age that a file must be in order to be pulled; any file older than this amount of time (according to last modification date) will be ignored': '文件被拖动的最大时间; 任何比此时间（根据上次修改日期）更早的文件将被忽略',
  'If true, the file is not deleted after it has been copied to the Content Repository; this causes the file to be picked up continually and is useful for testing purposes.  If not keeping original NiFi will need write permissions on the directory it is pulling from otherwise it will ignore the file.': '如果为true，则文件在复制到内容存储库后不会被删除; 这会导致文件不断被拾取，对测试有用。 如果不保留原有的DSG dataFlow，则需要对其所在目录的写权限，否则将忽略该文件。',
  'Indicates whether or not hidden files should be ignored': '指示是否应该忽略隐藏文件',
  'Batch Size': '批量大小',
  'The maximum number of files to pull in each iteration': '在每次迭代中拉取的最大文件数量',
  'The maximum number of yields, if the yield strategy is selected, that should be attempted before failures to obtain a high-water mark are routed to the “active” relationship (routing never occurs if unset or less than zero)': '如果选择了产量策略，那么最大产量数量应该在失败获得最大阀值标记之前进行尝试，并被路由到“活动”关系（如果未设置或小于零，路由将不会发生）',
  'The initial value for the water mark if none currently exists for the feed.': '如果目前没有流程，则为水印的初始值。',
  'Name of the flow file property which is set to the current high-water mark value for use in subsequent processing and commit': '被设置为当前最大阀值标记值的流文件属性的名称，用于后续处理和提交',
  'Regular expression for attributes to be deleted from FlowFiles. Existing attributes that match will be deleted regardless of whether they are updated by this processor.': '要从FlowFiles中删除的属性的正则表达式。 无论是否由该处理器更新，匹配的现有属性都将被删除。',
  'The name to be given to this high-water mark, stored in the feed\'s metadata, which records the latest committed water mark value': '存储在流程元数据中的此最大阀值标记的名称，记录最新的承诺水位值',
  'Specifies what strategy should be followed when an attempt to obtain the latest high-water mark fails because another is flow already actively using it': '指定当试图获取最新的最大阀值标记失败时应该遵循的策略，因为另一个流已经在积极使用它',
  ' Users pick and choose different data tables and columns and apply functions to transform the data to their desired destination table': '用户选择并选择不同的数据表和列，并应用函数将数据转换为所需的目标表',

  Component: '组件',
  Alert: '告警',
  None: '无',
  'Last Checked': '上次检查日期',
  'No results found': '没有发现结果',
  Components: '组件',

  Jobs: '作业',
  'N/A': '无',
  'Start Time': '开始时间',
  'Run Time': '运行时间',
  FAIL: '失败',
  ABANDON: '遗弃',
  'Job Name': '作业名称',
  Action: '动作',
  Time: '时间',
  Permissions: '权限',

  'Alert Type': '告警类型',
  AlertState: '告警状态',
  Level: '级别',
  Type: '类型',
  Cleared: '清除',
  Name: '名称',
  Description: '描述',

  INFO: '信息',
  WARNING: '警告',
  MINOR: '次要',
  MAJOR: '主要',
  CRITICAL: '危险',
  FATAL: '致命',

  'Service Level Assessment': '服务水平协议',
  Created: '创建的',
  Create: '创建',
  CREATE: '创建',
  'Service Level Agreement Description': '服务水平协议描述',
  Assessment: '评定',
  'Assessment Details': '评定详情',
  'Obligation Assessment Message': '职责评定消息',
  'Metric Assessment Message': '指标评定消息',

  Failure: '失败',
  Warning: '告警',
  Success: '成功',

  Message: '消息',
  'SLA Name': '服务水平协议名称',
  Result: '结果',

  'Internal Kylo tasks': ' 内部任务',
  'Next Fire': '下次触发',
  'Cron Expression': 'CRON 表达式',
  PAUSE: '暂停',
  RESUME: '恢复',
  'FIRE NOW': '现在触发',
  'Scheduler Details': '调度器详情',
  'PAUSE SCHEDULER': '暂停调度器',
  'RESUME SCHEDULER': '恢复调度器',
  'Up Time': '运行时长',
  'Tasks Executed': '指定的任务',

  'Pivot Charts': '数据透视图表',
  'Filter Chart': '过滤图表',
  Limit: '限制',
  Update: '更新',

  'Loading feeds': '加载数据流程',
  'Feed Name': '数据流程名',
  Export: '导出',
  State: '状态',
  Category: '分类',
  'Last Updated': '最近更新',

  'Select a template': '选择一个模板',
  or: '或',
  import: '导入',
  'from file': '从文件中',
  More: '更多',
  ago: '以前',
  'An error occurred': '发现一个错误',
  Ok: '好的',
  Units: '单位',
  'Cron Expression for when you expect to receive this data': 'Cron表达式，用于您希望收到此数据的时间',
  'Number specifying the amount of time allowed after the Expected Delivery Time': '指定预期交付时间之后允许的时间量的编号',


  'Category Details': '分类详情',
  Active: '激活',
  'Last Modified': '最近修改',
  Loading: '加载',
  'Feed Processing deadline': '流程处理的最后期限',
  'Ensure a Feed processes data by a specified time': '确保流程按指定的时间处理数据',
  ChangeIcon: '改变图标',

  'Category Name': '分类名称',
  'System Name': '系统名称',
  'Hadoop Security Groups': 'Hadoop 安全组',
  Icon: '图标',
  'Change Icon': '更改图标',
  ParentDependency: '上级依赖',

  'Category Roles': '分类角色',
  'Roles Inherited by Feeds': '数据流程继承的角色',

  'Loading Tables': '加载表数据',
  Table: '表',
  Schema: '模式',

  'SLA Assessments': '服务水平协议',
  Conditions: '条件',
  'Add Condition': '添加条件',
  'Select New Condition': '选择条件',
  Actions: '动作',
  'Select New Action': '选择新动作',
  Delete: '删除',
  Cancel: '取消',
  'Save SLA': '保存服务水平协议',

  'Related Feeds': '关联数据流程',
  'Data Source Details': '数据源详情',
  'Database Connection URL': '数据库连接 URL',
  'Database Driver Class Name': '数据库驱动类名称',
  'Database Driver Location': '数据库驱动路径位置',
  'Database User': '数据库用户',
  Password: '密码',
  references: '引用',

  'Loading templates': '加载模板',
  'Template Name': '模板名称',
  Template: '模板',
  '# of Properties': '属性数量',

  'Choose import method': '选择导入方法',

  'Allows access to feeds and feed-related functions': '允许访问流程和流程相关的功能',
  'Access Feeds': '访问流程',
  'Allows access to feeds and their metadata': '允许访问流程及其元数据',
  'Edit Feeds': '编辑流程',
  'Allows creating, updating, enabling and disabling feeds': '允许创建，更新，启用和禁用流程',
  'Import Feeds': '导入流程',
  'Export Feeds': '导出流程',
  'Administer Feeds': '管理流程',
  'Access Categories': '访问类别',
  'Edit Categories': '编辑类别',
  'Administer Categories': '管理类别',
  'Access Templates': '访问模板',
  'Allows access to feed templates': '允许访问流程模板',
  'Edit Templates': '编辑模板',
  'Import Templates': '导入模板',
  'Export Templates': '导出模板',
  'Administer Templates': '管理模板',
  'Access Data Sources': '访问数据源',
  'Edit Data Sources': '编辑数据源',
  'Allows creating and editing data sources': '允许创建和编辑数据源',
  'Administer Data Sources': '管理数据源',
  'Access Tables': '访问数据表',
  'Access Visual Query': '访问可视化查询',
  'Access Service Level Agreements': '访问服务级别协议',
  'Allows access to service level agreements': '允许访问服务级别协议',
  'Edit Service Level Agreements': '编辑服务水平协议',
  'Allows creating and editing service level agreements': '允许创建和编辑服务级别协议',
  'Edit Service Level Agreement Email Templates': '编辑服务级别协议电子邮件模板',
  'Allows creating and editing service level agreement email templates': '允许创建和编辑服务级别协议电子邮件模板',
  'Access Global Search': '访问全局搜索',
  'Allows access to search all indexed columns': '允许访问搜索所有索引列',
  'Access Operational Information': '访问操作信息',
  'Allows access to operational information like active feeds, execution history, job and feed stats, health status, etc.': '允许访问活动流程，执行历史记录，作业和流程统计信息，健康状况等操作信息。',
  'Allows access to visual query data wrangler': '允许访问可视查询数据处理程序',
  'Allows listing and querying Hive tables': '允许列举和查询Hive表',
  'Allows getting data source details with sensitive info': '允许获取敏感信息的数据源细节',
  'Allows (a) access to data sources (b) viewing tables and schemas from a data source (c) using a data source in transformation feed': '允许（a）访问数据源（b）使用转换流程中的数据源查看来自数据源的表和模式（c）',
  'Allows enabling and disabling feed templates': '允许启用和禁用流程模板',
  'Allows exporting template definitions (.zip files)': '允许导出模板定义（.zip文件）',
  'Allows importing of previously exported templates (.xml and .zip files)': '允许导入以前导出的模板（.xml和.zip文件）',
  'Allows creating, updating, deleting and sequencing feed templates': '允许创建，更新，删除和排序流程模板',
  'Allows updating category metadata': '允许更新类别元数据',
  'Administer Operations': '管理操作',
  'Allows administration of operations, such as creating/updating alerts, restart/stop/abandon/fail jobs, start/pause scheduler, etc.': '允许管理操作，如创建/更新警报，重启/停止/放弃/失败作业，启动/暂停调度程序等。',
  'Access Users and Groups Support': '访问用户和组支持',
  'Allows access to user and group-related functions': '允许访问用户和组相关的功能',
  'Access Users': '访问用户',
  'Allows the ability to view existing users': '允许查看现有用户',
  'Administer Users': '管理用户',
  'Allows the ability to create, edit and delete users': '允许创建，编辑和删除用户',
  'Access Groups': '访问用户组',
  'Allows the ability to view existing groups': '允许查看现有的组',
  'Administer Groups': '管理组',
  'Allows the ability to create, edit and delete groups': '允许创建，编辑和删除组',
  'Access Encryption Services': '访问加密服务',
  'Allows the ability to encrypt and decrypt values': '允许加密和解密值的能力',
  'Access Kylo Metadata': '访问元数据',
  'Administer Kylo Metadata': '管理元数据',
  'Allows the ability to directly manage the data in the Kylo metadata store (edit raw metadata, create/update/delete extensible types, update feed status events)': '允许直接管理元数据存储中的数据（编辑原始元数据，创建/更新/删除可扩展类型，更新流程状态事件）',
  'Allows the ability to view and query directly the data in the Kylo metadata store, including extensible types': '允许直接查看和查询元数据存储中的数据，包括可扩展类型',
  'Allows creating, updating and deleting categories': '允许创建，更新和删除类别',
  'Allows access to categories and their metadata': '允许访问类别及其元数据',
  'Allows deleting feeds and editing feed metadata': '允许删除流程和编辑流程元数据',
  'Service supplying the implementations of the various metadata providers.': '服务提供各种元数据提供者的实现。',
  'Allows exporting feeds definitions with their associated templates (.zip files)': '允许导出流程定义及其关联的模板（.zip文件）',
  'Allows importing of previously exported feeds along with their associated templates (.zip files)': '允许导入之前导出的流程及其关联的模板（.zip文件）',
  'Access Feed Support': '访问流程支持',

  Floor: '低面',
  Round: '范围',
  Ceiling: '置顶',
  'To Radians': '弧度',
  'hours ago': '小时以前',
  'To Degrees': '度数',
  'Header?': '标题?',
  'Delimiter Char': '分隔符字符',
  'Quote Char': '引用字符',
  'Escape Char': '转义字符',
  'Escape character': '转义字符',
  'Character enclosing a quoted string': '包含引用字符串的字符',
  'Character separating fields': '字符分隔字段',
  'Whether file has a header.': '文件是否有标题',
  'Supports delimited text files with a field delimiter and optional escape and quote characters.': '支持使用字段分隔符和可选的转义和引号字符的分隔文本文件。',
  'Auto detect will attempt to infer delimiter from the sample file.': '自动检测将尝试从样本文件推断分隔符.',
  'Auto Detect?': '自动侦测?',
  'Standard Data Ingest': '标准数据采集',
  'Data Transformation': '数据转换',
  Manual: '手动',
  'Import from NiFi': '从DSG dataFlow中导入',
  'Import a NiFi template directly from the current environment': '直接从当前环境导入DSG dataFlow模板',
  'Import from a file': '从文件导入',
  'Import from a Kylo archive or NiFi template file': '从数据湖或DSG dataFlow模板文件导入',
  'Sample File': '示例文件',
  'NOT REGISTERED': '未注册',
  'Merge using primary key': '使用主键合并',
  'Input Directory': '输入目录',
  'File Filter': '文件过滤',
  'Upload...': '上传...',
  'Add Field Policies': '添加字段策略',
  'Add a new policy': '添加一个新策略',
  Standardization: '标准化',
  standardization: '标准化',
  Validation: '验证',
  validation: '验证',
  HANDLED: '已处理',
  ALL: '全部',
  Service: '服务',
  'Sla Violation': 'Sla 违反',
  'Access Control': '访问控制',
  Required: '必填',
  'The SLA Name.': 'SLA 名称.',
  'The SLA Description.': 'SLA 描述.',
  'The SLA Assessment and result.': 'SLA评估和结果.',
  'months ago': '月以前.',
  TriggerFeed: '触发流程',
  'Feed Precondition Event Service': '流程前提条件事件服务',
  'Service that manages preconditions which trigger feed execution': '管理触发流程执行的先决条件的服务',
  'Matching Execution Context Keys': '匹配执行上下文键',
  'Comma separated list of Execution context keys or key fragments that will be applied to each of the dependent feed execution context data set.  Only the execution context values starting with keys this set will be included in the flow file JSON content.   Any key (case insensitive) starting with one of these supplied keys will be included': '将应用于每个相关流程源执行上下文数据集的逗号分隔的执行上下文关键字或关键字片段列表。 只有以此键开头的执行上下文值才会包含在流文件JSON内容中。 将包括以这些提供的密钥之一开头的任何密钥（不区分大小写）',
  'Apply Validation Results to Flow': '将验证结果应用于流程',
  Destination: '目标',
  'Path Not Found Behavior': '找不到路径行为',
  'Indicates how to handle missing JSON path expressions when destination is set to \'flowfile-attribute\'. Selecting \'warn\' will generate a warning when a JSON path expression is not found.': '指示目标设置为\'flowfile-attribute\'时如何处理缺少的JSON路径表达式。 当找不到JSON路径表达式时，选择\'warn\'会产生警告。',
  'Return Type': '返回类型',
  'Initialize Parameters': '初始化参数',
  'Get Feed Name to Validate': '获取流程名称以验证',
  'Execute Validation Query': '执行验证查询',
  'Convert Query Results to JSON': '将查询结果转换为JSON',
  'Indicates the desired return type of the JSON Path expressions.  Selecting \'auto-detect\' will set the return type to \'json\' for a Destination of \'flowfile-content\', and \'scalar\' for a Destination of \'flowfile-attribute\'.': '指示JSON路径表达式的所需返回类型。 选择“自动检测”将为\'flowfile-content\'的目标设置返回类型为\'json\'，\'flowfile-attribute\'的目标为\'标量\'。',
  'Indicates whether the results of the JsonPath evaluation are written to the FlowFile content or a FlowFile attribute; if using attribute, must specify the Attribute Name property. If set to flowfile-content, only one JsonPath may be specified, and the property name is ignored.': '指示JsonPath评估的结果是写入FlowFile内容还是写入FlowFile属性; 如果使用属性，则必须指定属性名称属性。 如果设置为flowfile-content，则只能指定一个JsonPath，并忽略属性名称。',
  'Null Value Representation': '空值表示',
  invalidThresholdPercentage: '无效的阈值百分比',
  'Validated template for import': '经过验证的导入模板',
  'Validated Reusable Templates': '经过验证的可重用模板',
  'Validating feed template for import': '验证流程模板的导入',
  'Validated the NiFi template.': '验证了DSG dataFlow模板。',
  'Imported S3 Data Ingest into NiFi': '将导入的S3数据导入DSG dataFlow',
  'Importing the NiFi flow, S3 Data Ingest': '导入DSG dataFlow流程，S3数据导入',
  'Registered template with Kylo metadata.': '带有元数据的注册模板。',
  'Removed the temporary process group S3 Data Ingest_1522139526149 in NiFi': '在DSG dataFlow中删除临时进程组S3 Data Ingest_1522139526149',
  'Successfully imported and registered the template S3 Data Ingest': '成功导入并注册模板S3 Data Ingest',
  'Validation error: The template S3 Data Ingest is already registered': '验证错误：模板S3 Data Ingest已经注册',
  'Unable to import the template S3 Data Ingest It is already registered.': '无法导入模板S3 Data Ingest它已被注册。',
  'Validation error: The template Data Confidence Invalid Records is already registered': '验证错误：模板Data Confidence Invalid Records已被注册',
  'Imported Data Confidence Invalid Records into NiFi': '导入数据信任无效记录到DSG dataFlow。',
  'Importing the NiFi flow, Data Confidence Invalid Records': '导入DSG dataFlow流，数据可信度无效记录。',
  'Unable to import the template Data Ingest It is already registered.': '无法导入模板Data Ingest它已被注册。',
  'Imported Data Ingest into NiFi': '向DSG dataFlow导入Data Ingest',
  'Validation error: The template Data Transformation is already registered': '验证错误：模板数据转换已经注册',
  'Unable to import the template Data Transformation It is already registered.': '无法导入模板数据转换它已被注册。',
  'Importing the NiFi flow, Data Ingest': '导入DSG dataFlow流， Data Ingest',
  'Removed the temporary process group Data Ingest_1522140811398 in NiFi': '在DSG dataFlow中删除临时进程组Data Ingest_1522140811398',
  'Validation error: The template Data Ingest is already registered': '导入DSG dataFlow流，数据可信度无效记录。',
  'Removed the temporary process group Data Confidence Invalid Records_1522140530928 in NiFi': '验证错误：模板Data Ingest已经注册',
  'Successfully imported and registered the template Data Confidence Invalid Records': '成功导入并注册了模板Data Confidence Invalid Records',
  'Indicates the desired representation of JSON Path expressions resulting in a null value.': '指示导致空值的JSON路径表达式的所需表示形式。',
  'Validation error. Feed import error: Unable to import feed.': '验证错误。 流程导入错误：无法导入流程。',
  'Validation error. Feed import error. The zip file you uploaded is not valid feed export.': '验证错误。 流程导入错误。 您上传的zip文件不是有效的流程输出。',
  'Unable to import and register the feed.  Errors were found. Ensure you are trying to upload a valid feed export file and not a template export file.': '无法导入和注册流程。 发现错误。 确保您试图上传有效的流程输出文件，而不是模板输出文件。',
  'Display Name': '显示名称',
  'Email Address': '邮件地址',
  Output: '输出',
  Quality: '质量目录',
  'Choose to decode and return as a binary(byte[]) or as a string': '选择解码并作为二进制（byte []）或字符串返回',
  'Base64 decode a string or a byte[].  Strings are evaluated using the UTF-8 charset': 'Base64解码一个字符串或一个字节[]。 字符串使用UTF-8字符集进行评估',
  'Attributes List': '属性列表',
  'Include Core Attributes': '包含核心属性',
  'Null Value': '空值',
  'Avro schema': 'Avro模式',
  'ZooKeeper Connection Timeout': 'ZooKeeper连接超时',
  'ZooKeeper Client Timeout': 'ZooKeeper客户端超时',
  Username: '用户名',
  'Solr Type': 'Solr类型',
  'Module Directory': '模块目录',
  'Script Body': '脚本正文',
  'Script Engine': '脚本引擎',
  'The engine to execute scripts': '引擎执行脚本',
  'JsonPath Expression': 'JsonPath表达式',
  'Character Set': '字符集',
  'Password to access the Elasticsearch cluster': '访问Elasticsearch集群的密码',
  'SSL Context Service': '访问Elasticsearch集群的密码',
  'The name of the index to insert into': '要插入的索引的名称',
  'The Solr collection name, only used with a Solr Type of Cloud': 'Solr集合名称仅用于Solr云类型',
  'The number of milliseconds before the given update is committed': '提交给定更新之前的毫秒数',
  'The path in Solr to post the ContentStream': 'Solr中发布ContentStream的路径',
  'Content-Type being sent to Solr': '内容类型被发送到Solr',
  'Hadoop configuration files': 'Hadoop配置文件',
  'Database Connection Pooling Service': '数据库连接池服务',
  'SQL select query': 'SQL select查询',
  'Columns to Return': '要返回的列',
  'Table Name': '表名',
  'Detect Search Engine': '检测搜索引擎',
  'Extract Feed Data': '提取流程数据',
  'Extract Payload': '提取有效负载',
  'Hash Data (Elasticsearch)': 'Hash数据(Elasticsearch)',
  'Prepare Search Engine Format': '准备搜索引擎格式',
  'Set Document Identifier (Elasticsearch)': '设置文档标识符（Elasticsearch）',
  'Set Document Identifier (Solr)': '设置文档标识符（Solr）',
  'Set Parameters': '设置参数',
  'Split Records (Elasticsearch)': '分割记录（Elasticsearch）',
  'Split Records (Solr)': '分割记录（Solr）',
  'Update Elasticsearch': '更新Elasticsearch',
  'Update Solr': '更新Solr',
  'Whether to change non-Avro-compatible characters in column names to Avro-compatible characters. For example, colons and periods will be changed to underscores in order to build a valid Avro record.': '是否将列名中的非Avro兼容字符更改为Avro兼容字符。 例如，冒号和句点将更改为下划线，以便构建有效的Avro记录。',
  'The number of result rows to be fetched from the result set at a time. This is a hint to the driver and may not be honored and/or exact. If the value specified is zero, then the hint is ignored.': '一次从结果集中获取的结果行数。 这是对驾驶员的暗示，可能不被尊重和/或确切。 如果指定的值为零，则提示将被忽略。',
  'If destination is Topic if present then make it the consumer shared. @see https://docs.oracle.com/javaee/7/api/javax/jms/Session.html#createSharedConsumer-javax.jms.Topic-java.lang.String-': '如果目的地是主题（如果存在），则使其成为消费者共享。 @请参阅https://docs.oracle.com/javaee/7/api/javax/jms/Session.html#createSharedConsumer-javax.jms.Topic-java.lang.String-',
  'The system name of this feed. The default is to have this name automatically set when the feed is created. Normally you do not need to change the default value.': '此流程的系统名称。 默认设置是在创建流程时自动设置此名称。 通常你不需要改变默认值。',
  'The category name of this feed. The default is to have this name automatically set when the feed is created. Normally you do not need to change the default value.': '此流程的类别名称。 默认设置是在创建流程时自动设置此名称。 通常你不需要改变默认值。',
  'Specifies whether to use S3 versions, if applicable.  If false, only the latest version of each object will be returned.': '指定是否使用S3版本（如果适用）。 如果为false，则只返回每个对象的最新版本。',
  'The prefix used to filter the object list. In most cases, it should end with a forward slash (\'/\').': '用于过滤对象列表的前缀。 在大多数情况下，它应该以正斜杠（\'/\'）结尾。',
  'Specifies whether to use the original List Objects or the newer List Objects Version 2 endpoint.': '指定是使用原始列表对象还是使用较新的列表对象版本2端点。',
  'The string used to delimit directories within the bucket. Please consult the AWS documentation for the correct use of this field.': '用于分隔存储桶中的目录的字符串。 请查阅AWS文档以正确使用此字段。',
  'The AWS libraries use the default signer but this property allows you to specify a custom signer to support older S3-compatible services.': 'AWS库使用默认的签名者，但是此属性允许您指定自定义签名者以支持较早的S3兼容服务。',
  'Specifies an optional SSL Context Service that, if provided, will be used to create connections': '指定一个可选的SSL上下文服务，如果提供该服务，将用于创建连接',
  'Proxy host port': '代理主机端口',
  'Proxy host name or IP': '代理主机名称或IP',
  'Endpoint URL to use instead of the AWS default including scheme, host, port, and path. The AWS libraries select an endpoint URL based on the AWS region, but this property overrides the selected endpoint URL, allowing use with other S3-compatible endpoints.': '要使用的端点URL而不是AWS默认值，包括方案，主机，端口和路径。 AWS库根据AWS区域选择一个端点URL，但该属性会覆盖选定的端点URL，从而允许与其他S3兼容端点一起使用。',
  'Path to a file containing AWS access key and secret key in properties file format.': '包含属性文件格式的AWS访问密钥和密钥的文件的路径。',
  'The Controller Service that is used to obtain aws credentials provider': '用于获取aws凭证提供程序的Controller服务',
  'The Solr url for a Solr Type of Standard (ex: http://localhost:8984/solr/gettingstarted), or the ZooKeeper hosts for a Solr Type of Cloud (ex: localhost:9983).': 'Solr标准类型的Solr URL（例如：http：// localhost：8984 / solr / gettingstarted）或Solr云类型的ZooKeeper主机（例如：localhost：9983）。',
  'The name of the FlowFile attribute containing the identifier for the document. If the Index Operation is "index", this property may be left empty or evaluate to an empty value, in which case the document\'s identifier will be auto-generated by Elasticsearch. For all other Index Operations, the attribute must evaluate to a non-empty value.': '包含文档标识符的FlowFile属性的名称。 如果索引操作是“索引”，则此属性可能会留空或评估为空值，在这种情况下，文档的标识符将由Elasticsearch自动生成。 对于所有其他的索引操作，该属性必须评估为非空值。',
  'Whether to use the first line as a header': '是否使用第一行作为标题',
  'Outgoing Avro schema for each record created from a CSV row': '从CSV行创建的每个记录的传出Avro模式',
  'Number of lines to skip before reading header or data': '阅读标题或数据之前要跳过的行数',
  'Quote character for CSV values': '引用CSV值的字符',
  'Escape character for CSV values': '转义CSV值的字符',
  'Character set for CSV files': 'CSV文件的字符集',
  'Value to be placed in the Avro record schema "name" field. The value must adhere to the Avro naming rules for fullname. If Expression Language is present then the evaluated value must adhere to the Avro naming rules.': '要放在Avro记录架构“名称”字段中的值。 该值必须符合Avro命名规则的全名。 如果表达语言存在，则评估值必须遵守Avro命名规则。',
  'This property only applies to CSV content type. String that represents a literal quote character in the CSV FlowFile content data.': '此属性仅适用于CSV内容类型。 表示CSV FlowFile内容数据中的文字引号字符的字符串。',
  'Delimiter character for CSV records': 'CSV记录的分隔符',
  'Character encoding of CSV data.': 'CSV数据的字符编码。',
  'This property only applies to CSV content type. If "true" the processor will attempt to read the CSV header definition from the first line of the input data.': '此属性仅适用于CSV内容类型。 如果“真”，处理器将尝试从输入数据的第一行读取CSV标题定义。',
  'Content Type of data present in the incoming FlowFile\'s content. Only "json" or "csv" are supported. If this value is set to "use mime.type value" the incoming Flowfile\'s attribute "MIME_TYPE" will be used to determine the Content Type.': '内容传入FlowFile内容中存在的数据类型。 只支持“json”或“csv”。 如果此值设置为“使用mime.type值”，则传入的Flowfile属性“MIME_TYPE”将用于确定内容类型。',
  'This property only applies to JSON content type. The number of JSON records that should be examined to determine the Avro schema. The higher the value the better chance kite has of detecting the appropriate type. However the default value of 10 is almost always enough.': '该属性仅适用于JSON内容类型。 应该检查以确定Avro模式的JSON记录的数量。 值越高，风筝检测适当类型的机会越好。 然而，默认值10几乎总是足够的。',
  'If true the Avro output will be formatted.': '如果为真，Avro输出将被格式化。',
  'Control if Avro schema is written as a new flowfile attribute \'inferred.avro.schema\' or written in the flowfile content. Writing to flowfile content will overwrite any existing flowfile content.': '控制Avro模式是作为新的流文件属性“inferred.avro.schema”写入还是写入流文件内容。 写入流文件内容将覆盖任何现有的流文件内容。',
  'This property only applies to CSV content type. Comma separated string defining the column names expected in the CSV data. EX: "fname,lname,zip,address". The elements present in this string should be in the same order as the underlying data. Setting this property will cause the value of "Get CSV Header Definition From Data" to be ignored instead using this value.': '此属性仅适用于CSV内容类型。 用逗号分隔的字符串定义CSV数据中预期的列名称。 EX：“fname，lname，zip，地址”。 此字符串中的元素应与底层数据的顺序相同。 设置此属性将导致“使用此值从数据获取CSV标头定义”的值被忽略。',
  'This property only applies to CSV content type. Specifies the number of lines that should be skipped when reading the CSV data. Setting this value to 0 is equivalent to saying "the entire contents of the file should be read". If the property "Get CSV Header Definition From Data" is set then the first line of the CSV  file will be read in and treated as the CSV header definition. Since this will remove the header line from the data care should be taken to make sure the value of "CSV header Line Skip Count" is set to 0 to ensure no data is skipped.': '此属性仅适用于CSV内容类型。 指定读取CSV数据时应跳过的行数。 将此值设置为0相当于说“应该读取文件的全部内容”。 如果属性“Get CSV Header Definition From Data”被设置，则CSV文件的第一行将被读入并且被视为CSV头部定义。 由于这将从数据中删除标题行，因此应注意确保“CSV标题行跳转计数”的值设置为0以确保不会跳过数据。',
  'This property only applies to CSV content type. String that represents an escape sequence in the CSV FlowFile content data.': '此属性仅适用于CSV内容类型。 表示CSV FlowFile内容数据中转义序列的字符串。',
  'Compression type to use when writting Avro files. Default is Snappy.': '编写Avro文件时使用的压缩类型。 默认是Snappy。',
  'Outgoing Avro schema for each record created from a JSON object': '从JSON对象创建的每个记录的传出Avro模式',
  'A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a \'core-site.xml\' and \'hdfs-site.xml\' file or will revert to a default configuration.': '包含Hadoop文件系统配置的文件或逗号分隔的文件列表。 否则，Hadoop将在类路径中搜索\'core-site.xml\'和\'hdfs-site.xml\'文件，或者恢复为默认配置。',
  'The maximum number of connections allowed from the Solr client to a single Solr host.': '允许从Solr客户端连接到单个Solr主机的最大连接数。',
  'The maximum number of total connections allowed from the Solr client to Solr.': '允许从Solr客户端到Solr的最大连接总数。',
  'The amount of time to wait when establishing a connection to Solr. A value of 0 indicates an infinite timeout.': '建立与Solr的连接时等待的时间量。 值为0表示无限超时。',
  'The Controller Service to use in order to obtain an SSL Context. This property must be set when communicating with a Solr over https.': '用于获取SSL上下文的Controller服务。 通过https与Solr通信时，必须设置此属性。',
  'The password to use when Solr is configured with basic authentication.': '使用基本身份验证配置Solr时使用的密码。',
  'The name of the JAAS configuration entry to use when performing Kerberos authentication to Solr. If this property is not provided, Kerberos authentication will not be attempted. The value must match an entry in the file specified by the system property java.security.auth.login.config.': '在对Solr执行Kerberos身份验证时使用的JAAS配置条目的名称。 如果未提供此属性，则不会尝试Kerberos身份验证。 该值必须与系统属性java.security.auth.login.config指定的文件中的条目匹配。',
  'The SSL Context Service used to provide client certificate information for TLS/SSL connections. This service only applies if the Elasticsearch endpoint(s) have been secured with TLS/SSL.': 'SSL上下文服务用于为TLS / SSL连接提供客户端证书信息。 此服务仅适用于Elasticsearch端点已使用TLS / SSL进行保护的情况。',
  'Path to script file to execute. Only one of Script File or Script Body may be used': '脚本文件的执行路径。 只能使用脚本文件或脚本正文中的一个',
  'The name of the FlowFile Attribute into which the Hash Value should be written. If the value already exists, it will be overwritten': '应该写入散列值的FlowFile属性的名称。 如果该值已经存在，它将被覆盖',
  'Determines what hashing algorithm should be used to perform the hashing function': '确定应使用什么散列算法来执行散列函数',
  'User Name used for authentication and authorization.': '用户名用于认证和授权。',
  'The name of the subscription to use if destination is Topic and is shared or durable.': '如果目标是主题并且是共享的或持久的，则使用该订阅的名称。',
  'The maximum limit for the number of cached Sessions.': '缓存的会话数量的最大限制。',
  'Password used for authentication and authorization.': '用于认证和授权的密码。',
  'If destination is Topic if present then make it the consumer durable. @see https://docs.oracle.com/javaee/7/api/javax/jms/Session.html#createDurableConsumer-javax.jms.Topic-java.lang.String-': '如果目的地是主题（如果存在），则使其成为耐用消费者。 @see https://docs.oracle.com/javaee/7/api/javax/jms/Session.html#createDurableConsumer-javax.jms.Topic-java.lang.String-',
  'The type of the JMS Destination. Could be one of \'QUEUE\' or \'TOPIC\'. Usually provided by the administrator. Defaults to \'TOPIC': 'JMS目标的类型。 可能是\'QUEUE\'或\'TOPIC\'之一。 通常由管理员提供。 默认为\'TOPIC',
  'The name of the JMS Destination. Usually provided by the administrator (e.g., \'topic://myTopic\' or \'myTopic\').': 'JMS目标的名称。 通常由管理员提供（例如，\'topic：// myTopic\'或\'myTopic\'）。',
  'The Controller Service that is used to obtain ConnectionFactory': '用于获取ConnectionFactory的Controller服务',
  'The client id to be set on the connection, if set. For durable non shared consumer this is mandatory, for all others it is optional, typically with shared consumers it is undesirable to be set. Please see JMS spec for further details': '在连接上设置的客户端ID（如果已设置）。 对于持久的非共享消费者，这是强制性的，对于所有其他消费者是可选的，通常对于共享消费者而言，不希望被设置。 有关更多详细信息，请参阅JMS规范',
  'The JMS Acknowledgement Mode. Using Auto Acknowledge can cause messages to be lost on restart of NiFi but may provide better performance than Client Acknowledge.': 'JMS确认模式。 使用自动确认可能会导致重新启动DSG dataFlow时丢失消息，但可能会提供比客户端确认更好的性能。',
  'Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to \'failure\' or \'retry\' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this \'Rollback On Failure\' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate \'Yield Duration\' to avoid retrying too frequently.NOTE: When an error occurred after a Hive streaming transaction which is derived from the same input FlowFile is already committed, (i.e. a FlowFile contains more records than \'Records per Transaction\' and a failure occurred at the 2nd transaction or later) then the succeeded records will be transferred to \'success\' relationship while the original input FlowFile stays in incoming queue. Duplicated records can be created for the succeeded ones when the same FlowFile is processed again.': '指定如何处理错误。默认情况下（false），如果在处理FlowFile时发生错误，FlowFile将根据错误类型被路由到\'失败\'或\'重试\'关系，并且处理器可以继续下一个FlowFile。相反，您可能想要回滚当前处理的FlowFiles并立即停止进一步的处理。在这种情况下，您可以通过启用“回滚失败”属性来完成此操作。如果启用，失败的FlowFiles将保留在输入关系中，而不会对其进行处罚并重复处理，直到成功处理或通过其他方式删除。设置足够的“Yield Duration”以避免重试过于频繁是非常重要的。注意：如果在已经提交了来自相同输入FlowFile的Hive流业务之后发生错误（例如，FlowFile包含的记录数多于\'Records per事务\'并且在第二个业务或更晚的事件中发生故障），那么当原始输入FlowFile停留在传入队列中时，成功的记录将被转移到\'成功\'关系。当同一个FlowFile被再次处理时，可以为成功的记录创建重复的记录。',
  'A hint to Hive Streaming indicating how many transactions the processor task will need. This value must be greater than 1.': '提示Hive Streaming，表明处理器任务需要多少业务。 该值必须大于1。',
  'The name of the database table in which to put the data.': '数据库表的名称，用于放置数据。',
  'Number of records to process before committing the transaction. This value must be greater than 1.': '提交业务之前要处理的记录数。 该值必须大于1。',
  'A comma-delimited list of column names on which the table has been partitioned. The order of values in this list must correspond exactly to the order of partition columns specified during the table creation.': '表格已被分区的列名的逗号分隔列表。 此列表中值的顺序必须与表创建期间指定的分区列的顺序完全一致。',
  'The URI location for the Hive Metastore. Note that this is not the location of the Hive Server. The default port for the Hive metastore is 9043.': 'Hive Metastore的URI位置。 请注意，这不是Hive Server的位置。 Hive Metastore的默认端口是9043。',
  'The maximum number of open connections that can be allocated from this pool at the same time, or negative for no limit.': '可同时从该池中分配的最大打开连接数，或无限制的负数。',
  'Indicates that a heartbeat should be sent when the specified number of seconds has elapsed. A value of 0 indicates that no heartbeat should be sent. Note that although this property supports Expression Language, it will not be evaluated against incoming FlowFile attributes.': '指示在经过指定的秒数后应该发送心跳。 值为0表示不应发送心跳。 请注意，虽然此属性支持表达式语言，但它不会针对传入的FlowFile属性进行评估。',
  'The name of the database in which to put the data.': '要放置数据的数据库的名称。',
  'The number of seconds allowed for a Hive Streaming operation to complete. A value of 0 indicates the processor should wait indefinitely on operations. Note that although this property supports Expression Language, it will not be evaluated against incoming FlowFile attributes.': 'Hive流操作完成所允许的秒数。 值为0表示处理器应该无限期地等待操作。 请注意，虽然此属性支持表达式语言，但它不会针对传入的FlowFile属性进行评估。',
  'Flag indicating whether partitions should be automatically created': '指示是否应该自动创建分区的标志',
  'A file or comma separated list of files which contains the Hive configuration (hive-site.xml, e.g.). Without this, Hadoop will search the classpath for a \'hive-site.xml\' file or will revert to a default configuration. Note that to enable authentication with Kerberos e.g., the appropriate properties must be set in the configuration files. Also note that if Max Concurrent Tasks is set to a number greater than one, the \'hcatalog.hive.client.cache.disabled\' property will be forced to \'true\' to avoid concurrency issues. Please see the Hive documentation for more details.': '包含Hive配置的文件或逗号分隔的文件列表（例如，hive-site.xml）。 如果没有这个，Hadoop将在类路径中搜索\'hive-site.xml\'文件或恢复为默认配置。 请注意，要启用Kerberos身份验证，例如必须在配置文件中设置适当的属性。 还要注意，如果Max Concurrent Tasks设置为大于1的数字，\'hcatalog.hive.client.cache.disabled\'属性将被强制为\'true\'以避免并发问题。 请参阅Hive文档以获取更多详细信息。',
  'Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties': 'Kerberos主体进行身份验证。 需要在您的DSG dataFlow.properties中设置DSG dataFlow.kerberos.krb5.file',
  'Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties': '与主体关联的Kerberos密钥表。 需要在您的DSG dataFlow.properties中设置DSG dataFlow.kerberos.krb5.file',
  'The maximum number of result rows that will be included in a single FlowFile. This will allow you to break up very large result sets into multiple FlowFiles. If the value specified is zero, then all rows are returned in a single FlowFile.': '将包含在单个FlowFile中的最大结果行数。 这将允许您将非常大的结果集分解为多个FlowFiles。 如果指定的值为零，则所有行都将返回到单个FlowFile中。',
  'The maximum number of fragments. If the value specified is zero, then all fragments are returned. This prevents OutOfMemoryError when this processor ingests huge table.': '片段的最大数量。 如果指定的值为零，则返回所有片段。 当这个处理器摄入巨大的表时，这可以防止OutOfMemoryError。',
  'Whether to use Avro Logical Types for DECIMAL/NUMBER, DATE, TIME and TIMESTAMP columns. If disabled, written as string. If enabled, Logical types are used and written as its underlying type, specifically, DECIMAL/NUMBER as logical \'decimal\': written as bytes with additional precision and scale meta data, DATE as logical \'date-millis\': written as int denoting days since Unix epoch (1970-01-01), TIME as logical \'time-millis\': written as int denoting milliseconds since Unix epoch, and TIMESTAMP as logical \'timestamp-millis\': written as long denoting milliseconds since Unix epoch. If a reader of written Avro records also knows these logical types, then these values can be deserialized with more context depending on reader implementation.': '是否对DECIMAL / NUMBER，DATE，TIME和TIMESTAMP列使用Avro逻辑类型。 如果禁用，则写为字符串。 如果启用，则使用逻辑类型并将其编写为其基础类型，具体而言，DECIMAL / NUMBER作为逻辑\'decimal\'：写为具有附加精度和缩放元数据的字节，DATE作为逻辑\'date-millis\'：写为int，表示天 自从Unix纪元（1970-01-01）以来，TIME被视为逻辑上的\'time-millis\'：写成int表示从Unix纪元开始的毫秒数，TIMESTAMP作为逻辑\'timestamp-millis\'：写成自从Unix纪元以来的长时间表示毫秒。 如果一个写Avro记录的读者也知道这些逻辑类型，那么这些值可以用更多的上下文进行反序列化，这取决于读者的实现。',
  'When a DECIMAL/NUMBER value is written as a \'decimal\' Avro logical type, a specific \'scale\' denoting number of available decimal digits is required. Generally, scale is defined by column data type definition or database engines default. However when undefined precision (0) is returned, scale can also be uncertain with some database engines. \'Default Decimal Scale\' is used when writing those undefined numbers. If a value has more decimals than specified scale, then the value will be rounded-up, e.g. 1.53 becomes 2 with scale 0, and 1.5 with scale 1.': '当DECIMAL / NUMBER值被写为\'十进制\'Avro逻辑类型时，需要一个特定的\'scale\'来表示可用的十进制数字的数量。 通常，缩放由列数据类型定义或数据库引擎默认值定义。 但是，当返回未定义的精度（0）时，对于某些数据库引擎，缩放也可能不确定。 在写入这些未定义的数字时使用\'Default Decimal Scale\'。 如果一个数值的小数多于指定比例，那么该值将被四舍五入，例如， 1.53变为2，刻度为0,1.5为刻度1。',
  'When a DECIMAL/NUMBER value is written as a \'decimal\' Avro logical type, a specific \'precision\' denoting number of available digits is required. Generally, precision is defined by column data type definition or database engines default. However undefined precision (0) can be returned from some database engines. \'Default Decimal Precision\' is used when writing those undefined precision numbers.': '当DECIMAL / NUMBER值被写为\'十进制\'Avro逻辑类型时，需要一个特定的“精度”来表示可用数字的数量。 通常，精度由列数据类型定义或数据库引擎默认值定义。 然而，未定义的精度（0）可以从一些数据库引擎返回。 在写入这些未定义的精度数字时使用\'Default Decimal Precision\'。',
  'The name of the database table to be queried.': '要查询的数据库表的名称。',
  'The type/flavor of database, used for generating database-specific code. In many cases the Generic type should suffice, but some databases (such as Oracle) require custom SQL clauses.': '数据库的类型/风格，用于生成特定于数据库的代码。 在许多情况下，泛型类型应该就足够了，但是一些数据库（如Oracle）需要定制SQL子句。',
  'A comma-separated list of column names. The processor will keep track of the maximum value for each column that has been returned since the processor started running. Using multiple columns implies an order to the column list, and each column\'s values are expected to increase more slowly than the previous columns\' values. Thus, using multiple columns implies a hierarchical structure of columns, which is usually used for partitioning tables. This processor can be used to retrieve only those rows that have been added/updated since the last retrieval. Note that some JDBC types such as bit/boolean are not conducive to maintaining maximum value, so columns of these types should not be listed in this property, and will result in error(s) during processing. If no columns are provided, all rows from the table will be considered, which could have a performance impact. NOTE: It is important to use consistent max-value column names for a given table for incremental fetch to work properly.': '列名称的逗号分隔列表。 处理器将跟踪自处理器开始运行以来返回的每列的最大值。 使用多列意味着列列表的顺序，并且每列的值预计将比前一列的值增加得更慢。 因此，使用多列意味着列的层次结构，通常用于分区表。 该处理器可用于仅检索自上次检索后添加/更新的那些行。 请注意，某些JDBC类型（如bit / boolean）不利于维护最大值，因此这些类型的列不应列在此属性中，并且会在处理期间导致错误。 如果未提供列，则会考虑表中的所有行，这可能会对性能产生影响。 注：对于给定表使用一致的最大值列名以使增量提取正常工作非常重要。',
  'A comma-separated list of column names to be used in the query. If your database requires special treatment of the names (quoting, e.g.), each name should include such treatment. If no column names are supplied, all columns in the specified table will be returned. NOTE: It is important to use consistent column names for a given table for incremental fetch to work properly.': '查询中要使用的列名称的逗号分隔列表。 如果您的数据库需要对名称进行特殊处理（例如引用），则每个名称都应包含此类处理。 如果未提供列名，则将返回指定表中的所有列。 注：对于给定表使用一致的列名称以使增量提取正常工作非常重要。',
  'The Controller Service that is used to obtain connection to database': '用于获取数据库连接的Controller服务',
  'Specifies the Controller Service to use for writing out the records': '指定用于写出记录的Controller服务器',
  'The amount of time to wait for data on a socket connection to Solr. A value of 0 indicates an infinite timeout.': '等待到Solr的套接字连接上的数据的时间量。 值为0表示无限超时。',
  'The type of this document (used by Elasticsearch for indexing and searching)': '此文档的类型（由Elasticsearch用于索引和搜索）',
  'The type of the operation used to index (index, update, upsert, delete)': '用于索引的操作的类型（索引，更新，插入，删除）',
  'Max wait time for the connection to the Elasticsearch REST API.': '连接到Elasticsearch REST API的最长等待时间。',
  'Max wait time for a response from the Elasticsearch REST API.': '来自Elasticsearch REST API的响应的最长等待时间。',
  'Elasticsearch URL which will be connected to, including scheme (http, e.g.), host, and port. The default port for the REST API is 9200.': '将连接到的Elasticsearch URL，包括方案（http，例如），主机和端口。 REST API的默认端口是9200。',
  'The preferred number of flow files to put to the database in a single transaction. Note that the contents of the flow files will be stored in memory until the bulk operation is performed. Also the results should be returned in the same order the flow files were received.': '在单个业务中放入数据库的流文件的首选数量。 请注意，流程文件的内容将存储在内存中，直到执行批量操作。 此外，结果应该以流文件收到的相同顺序返回。',
  'Username to access the Elasticsearch cluster': '用户名来访问Elasticsearch集群',
  'Specifies the character set of the document data.': '指定文档数据的字符集。',
  'A JsonPath expression that indicates the array element to split into JSON/scalar fragments.': '一个JsonPath表达式，用于指示要拆分为JSON /标量片段的数组元素。',
  'Body of script to execute. Only one of Script File or Script Body may be used': '要执行的脚本的主体。 只能使用脚本文件或脚本正文中的一个',
  'Comma-separated list of paths to files and/or directories which contain modules required by the script.': '包含脚本所需模块的文件和/或目录的路径列表，以逗号分隔。',
  'Specifies the Controller Service to use for reading incoming data': '指定用于读取传入数据的Controller服务器',
  'Determines if the resulting output for empty records or a single record should be wrapped in a container array as specified by \'JSON container options\'': '确定空记录或单个记录的结果输出是否应按照\'JSON容器选项\'指定的容器数组包装在容器数组中',
  'The input directory from which to pull files': '从中提取文件的输入目录',
  'Only files whose names match the given regular expression will be picked up': '只有名称与给定正则表达式匹配的文件才会被拾取',
  'The type of Solr instance, Cloud or Standard.': 'Solr实例的类型，云或标准。',
  'The username to use when Solr is configured with basic authentication.': '使用基本身份验证配置Solr时使用的用户名。',
  'The amount of time to wait for data on a connection to ZooKeeper, only used with a Solr Type of Cloud.': '等待连接到ZooKeeper的数据的时间量，仅与Solr云类型一起使用。',
  'The amount of time to wait when establishing a connection to ZooKeeper, only used with a Solr Type of Cloud.': '建立到ZooKeeper的连接时等待的时间量，仅与Solr云类型一起使用。',
  'Determines how stream of records is exposed: either as a sequence of single Objects (none) (i.e. writing every Object to a new line), or as an array of Objects (array).': '确定如何公开记录流：作为单个对象的序列（无）（即将每个对象写入新行）或作为对象（数组）的数组。',
  'If the Avro records do not contain the schema (datum only), it must be specified here.': '如果Avro记录不包含模式（仅限数据），则必须在此指定。',
  'Regular expression that will be evaluated against the flow file attributes to select the matching attributes. This property can be used in combination with the attributes list property.': '正则表达式将根据流文件属性进行评估以选择匹配属性。 该属性可以与属性列表属性结合使用。',
  'If true a non existing or empty attribute will be NULL in the resulting JSON. If false an empty string will be placed in the JSON': '如果为true，则在生成的JSON中，非现有属性或空属性将为NULL。 如果为false，则将在JSON中放置一个空字符串',
  'Determines if the FlowFile org.apache.nifi.flowfile.attributes.CoreAttributes which are contained in every FlowFile should be included in the final JSON value generated.': '确定每个FlowFile中包含的FlowFile org.apache.DSG dataFlow.flowfile.attributes.CoreAttributes是否应包含在生成的最终JSON值中。',
  'Control if JSON value is written as a new flowfile attribute \'JSONAttributes\' or written in the flowfile content. Writing to flowfile content will overwrite any existing flowfile content.': '如果JSON值被写为新的流文件属性\'JSONAttributes\'或写入流文件内容，请进行控制。 写入流文件内容将覆盖任何现有的流文件内容。',
  'Comma separated list of attributes to be included in the resulting JSON. If this value is left empty then all existing Attributes will be included. This list of attributes is case sensitive. If an attribute specified in the list is not found it will be be emitted to the resulting JSON with an empty string or NULL value.': '逗号分隔的属性列表将包含在生成的JSON中。 如果该值保留为空，则将包括所有现有的属性。 该属性列表区分大小写。 如果没有找到列表中指定的属性，它将被发送到生成的JSON，并使用空字符串或NULL值。',
  'Data Ingest': '数据采集',
  'Base64 Decode': 'Base64解码',
  'Base64 decode a string or a byte[]. Strings are evaluated using the UTF-8 charset': 'Base64解码一个字符串或一个字节[].字符串使用UTF-8字符集进行评估.',
  BINARY: '二进制',
  STRING: '字符串',
  'Choose to encode and return as a binary (byte[]) or as a string': '选择编码并返回为二进制（byte []）或字符串.',
  'Base64 Encode': 'Base64编码',
  'Base64 encode a string or a byte[].  Strings are evaluated using the UTF-8 charset.  String output is urlsafe': 'Base64编码一个字符串或一个字节[]。 字符串使用UTF-8字符集进行评估。 字符串输出是urlsafe',
  'Control characters': '控制字元',
  'Remove Control Characters': '删除控制字符',
  'Date/Time': '日期／时间',
  'Converts any date to Hive-friendly format with optional timezone conversion': '将任何日期转换为可选的时区转换格式.',
  'Date Format': '日期格式',
  'Format Example: MM/dd/YYYY.  If converting from Unix timestamp leave empty.': '格式示例:MM/dd/YYYY.如果从Unix时间戳转换为空.',
  'Output Format': '输出格式',
  DATE_ONLY: '只有日期',
  DATETIME: '日期时间',
  'Choose an output format': '选择一种输出格式',
  'Input timezone': '输入时区',
  'Input timezone (optional)': '输入时区(可选)',
  'Output timezone': '输出时区',
  'Targeted timezone (optional)': '目标时区（可选）(可选)',
  'Default Value': '默认值',
  'Applies a default value if null': '如果null，则应用默认值.',
  'If the value is null it will use this supplied value': '如果值为空，它将使用这个提供的值.',
  Lowercase: '小写',
  'Convert string to lowercase': '将字符串转换为小写',
  'Preserves last 4 digits': '保留最后4位数字',
  'Regex Replacement': '正则表达式替换',
  'Replace text based upon a regex pattern': '根据正则表达式模式替换文本',
  'Regex Pattern': '正则表达式模式',
  Replacement: '替换',
  'Text to replace the regex match': '文本替换正则表达式匹配',
  'Strip Non Numeric': '带非数值',
  'Remove any characters that are not numeric': '删除任何不是数字的字符',
  'Trim the leading and trailing spaces on a string': '去除字符串前后的空格',
  Uppercase: '大写',
  'Convert string to uppercase': '将字符串转换为大写',
  'Validate Hive-friendly date format': '验证Hive友好的日期格式',
  Date: '日期',
  Email: '电子邮箱',
  'Valid email address': '有效的电子邮箱地址',
  'IP Address': 'IP地址',
  'Valid IP Address': '有效的IP地址',
  Length: '长度',
  'Validate String falls between desired length': '验证字符串在所需长度之间.',
  'Max Length': '最大长度',
  'Min Length': '最小长度',
  Lookup: '查找',
  'Must be contained in the list': '必须包含在列表中',
  List: '列表',
  'Comma separated list of values': '值列表使用逗号分隔',
  'Not Null': '非空',
  'Validate a value is not null': '验证一个值不为空',
  'Allow Empty String Values': '允许空字符串值',
  No: '否',
  Yes: '是',
  'If the value is a String, are empty strings not null?': '如果该字段取值是字符串，是否允许字段的值为空字符串',
  'Trim String Values': '修剪字符串值',
  ' If the value is a String, should it be trimmed before checking for null?': '如果该字段取值是字符串，是否要在检查空之前去掉首尾空白字符',
  'If the value is a String, should it be trimmed before checking for null?': '如果该字段取值是字符串，是否要在检查空之前去掉首尾空白字符',
  Range: '范围',
  'Numeric must fall within range': '数值必须在范围内',
  Min: '最小',
  Max: '最大',
  'Minimum Value': '最小值',
  'Maximum Value': '最大值',
  Regex: '正则表达式',
  'Validate Regex Pattern': '验证正则表达式模式',
  'Regex expression': '正则表达式',
  Timestamp: '时间戳',
  'Validate Hive-friendly timstamp format': '验证Hive友好的时间戳格式',
  'Allow Null Values': '允许空值',
  'Null values are considered to be valid': '空值被认为是有效的',
  'US Phone': '美国电话',
  'Validate US Phone': '验证美国电话',
  'US Zip': '美国邮编',
  'Validate US Zip': '验证美国邮编',
  'Validate Characters': '验证字符',
  'Validate common character functions such as upper/lower/alpha numeric..': '验证常见的字符函数，例如上/下/字母数字等.',
  'Validation Type': '验证类型',
  AlphaNumeric: '字母数字',
  Alpha: '字母',
  Numeric: '数字',
  'The type of character validation to perform': '执行的字符验证类型',
  Aggregate: '聚合函数',
  Conditional: '条件函数',
  Pivot: '透视聚合函数',
  Window: '窗口函数',
  Owner: '所有者',
  'Allows a user to edit, enable/disable, delete and export feed. Allows access to job operations for feed. If role inherited via a category, allows these operations for feeds under that category.': '允许用户编辑、启用/禁用、删除和导出提要。允许访问作业操作以获取提要。如果角色是通过一个类别继承的，那么允许在该类别下进行这些操作',
  Editor: '编辑',
  'Read-Only': '只读',
  'All capabilities defined in the \'Editor\' role along with the ability to change the permissions': '在“编辑”角色中定义的所有功能以及更改权限的能力',
  'Allows a user to view the feed and access job operations': '允许用户查看提要和访问作业操作',
  'Add User/Group access': '添加用户/组的访问',
  'es-username': 'ES用户名',
  'es-password': 'ES密码',
  'index-or-collection-name': 'ES 索引名',
  'es-type': 'ES索引类别',
  'Connection Mode': '连接方式',
  'The FTP Connection Mode': 'FTP连接方式',
  'Connection Timeout': '连接超时',
  'Amount of time to wait before timing out while creating a connection': '在创建连接之前超时等待的时间量',
  'Data Timeout': '数据超时',
  'Delete Original': '删除原件',
  'File Filter Regex': '文件过滤正则表达式',
  Hostname: '主机名',
  'Http Proxy Password': 'HTTP代理密码',
  'Http Proxy Username': 'HTTP代理用户名',
  'Ignore Dotted Files': '忽略点文件',
  'Internal Buffer Size': '内部缓冲区大小',
  'Set the internal buffer size for buffered data streams': '设置缓冲数据流的内部缓冲区大小',
  'Password for the user account': '用户帐户密码',
  'Path Filter Regex': '路径过滤正则表达式',
  'When Search Recursively is true, then only subdirectories whose path matches the given Regular Expression will be scanned': '当递归搜索为真时，将只扫描其路径与给定正则表达式匹配的子目录。',
  'Determines how long to wait between fetching the listing for new files': '确定在获取新文件的列表之间等待多长时间',
  'Proxy Host': '代理主机',
  'Proxy Port': '代理端口',
  'The port of the proxy server': '代理服务器的端口',
  'Proxy Type': '代理类型',
  'Proxy type used for file transfers': '用于文件传输的代理类型',
  'Remote Path': '远程路径',
  'Remote Poll Batch Size': '远程轮询批处理大小',
  'Search Recursively': '递归搜索',
  'Transfer Mode': '传输模式',
  'The FTP Transfer Mode': 'FTP传输模式',
  Port: '端口',
  'Catalog Name': '目录名称',
  'JDBC Connection Pool': 'JDBC连接池',
  'Schema Name': '架构名称',
  'Statement Type': '声明类型',
  'Translate Field Names': '翻译字段名称',
  'Unmatched Column Behavior': '不匹配列行为',
  'Unmatched Field Behavior': '不匹配字段行为',
  'Transaction Timeout': '业务超时',
  'Support Fragmented Transactions': '支持单个业务',
  'If true, any key that is automatically generated by the database will be added to the FlowFile that generated it using the sql.generate.key attribute. This may result in slightly slower performance and is not supported by all databases.': '如果为真，则由数据库自动生成的任何密钥将被添加到使用sql.generate.key属性生成的FlowFile。这可能会导致性能稍慢，并且不受所有数据库的支持。',
  'If true, when a FlowFile is consumed by this Processor, the Processor will first check the fragment.identifier and fragment.count attributes of that FlowFile. If the fragment.count value is greater than 1, the Processor will not process any FlowFile with that fragment.identifier until all are available; at that point, it will process all FlowFiles with that fragment.identifier as a single transaction, in the order specified by the FlowFiles\' fragment.index attributes. This Provides atomicity of those SQL statements. If this value is false, these attributes will be ignored and the updates will occur independent of one another.': '如果为真，当FlowFile被该处理器消耗时，处理器将首先检查该FlowFile的碎片。标识符和碎片。如果碎片值大于1，处理器将不处理任何带有该碎片的标识符，直到所有可用；在这一点上，它将按照FlowFile FlowFiles的指定顺序将该片段作为标识符处理所有FlowFiles。属性除外。这提供了这些SQL语句的原子性。如果此值为false，则这些属性将被忽略，并且更新将彼此独立地发生。',
  'If the <Support Fragmented Transactions> property is set to true, specifies how long to wait for all FlowFiles for a particular fragment.identifier attribute to arrive before just transferring all of the FlowFiles with that identifier to the \'failure\' relationship': '如果<Support Fragmented Transactions>属性设置为true，那么在将所有具有该标识符的FlowFiles转移到“失败”关系之前，指定等待特定fragment.identifier属性的所有FROWS文件要多长时间。',
  'Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to \'failure\' or \'retry\' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this \'Rollback On Failure\' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate \'Yield Duration\' to avoid retrying too frequently.': '指定如何处理错误。默认情况下（false），如果处理流程文件时出错，则流文件将根据错误类型被路由到“失败”或“重试”关系，并且处理器可以继续使用下一个FlowFile。相反，您可能希望回滚当前处理的流文件，并立即停止进一步处理。在这种情况下，您可以通过启用此“回滚失败”属性来实现这一点。如果启用，失败的FlowFiles将保持在输入关系中而不惩罚它并反复处理直到它成功地处理或通过其他方式被移除。设置足够的“产量持续时间”以避免过于频繁地重试是很重要的。',
  'The preferred number of FlowFiles to put to the database in a single transaction': '在单个业务中向数据库提交的首选FlowFiles数',
  'The string to be prepended to the outgoing flow file attributes, such as <sql>.args.1.value, where <sql> is replaced with the specified value': '将要添加到输出流文件属性的字符串，如<sql>.args.1.value，其中用指定值替换<SQL>',
  'Enabling this option will cause the table name to be quoted to support the use of special characters in the table name': '启用此选项将导致引用表名以支持在表名中使用特殊字符',
  'A comma-separated list of column names that uniquely identifies a row in the database for UPDATE statements. If the Statement Type is UPDATE and this property is not set, the table\'s Primary Keys are used. In this case, if no Primary Key exists, the conversion to SQL will fail if Unmatched Column Behaviour is set to FAIL. This property is ignored if the Statement Type is INSERT': '以逗号分隔的列名列表，用于唯一标识数据库中UPDATE语句的行。 如果语句类型为UPDATE且未设置此属性，则使用表的主键。 在这种情况下，如果不存在主键，如果“不匹配的列行为”设置为“失败”，则转换为SQL将失败。 如果语句类型为INSERT，则忽略此属性',
  'Enabling this option will cause all column names to be quoted, allowing you to use reserved words as column names in your tables.': '启用此选项将导致引用所有列名称，允许您在表中使用保留字作为列名。',
  'Update Keys': '更新密钥',
  'If an incoming JSON element has a field that does not map to any of the database table\'s columns, this property specifies how to handle the situation': '如果传入的JSON元素具有不映射到任何数据库表的列的字段，则此属性指定如何处理该情况',
  'If an incoming JSON element does not have a field mapping for all of the database table\'s columns, this property specifies how to handle the situation': '如果传入的JSON元素没有对所有数据库表列的字段映射，则该属性指定如何处理该情况。',
  'If true, the Processor will attempt to translate JSON field names into the appropriate column names for the table specified. If false, the JSON field names must match the column names exactly, or the column will not be updated': '如果为true，则处理器将尝试将JSON字段名称转换为指定表的相应列名称。 如果为false，则JSON字段名称必须与列名称完全匹配，否则不会更新列',
  'The name of the table that the statement should update': '声明应更新的表的名称',
  'Specifies the type of SQL Statement to generate': '指定要生成的SQL语句的类型',
  'The name of the schema that the table belongs to. This may not apply for the database that you are updating. In this case, leave the field empty': '表所属的架构的名称。 这可能不适用于您要更新的数据库。 在这种情况下，请将该字段留空',
  'Specifies the JDBC Connection Pool to use in order to convert the JSON message to a SQL statement. The Connection Pool is necessary in order to determine the appropriate database column types.': '指定要使用JDBC连接池，以便将JSON消息转换为SQL语句。为了确定适当的数据库列类型，连接池是必需的。',
  'The name of the catalog that the statement should update. This may not apply for the database that you are updating. In this case, leave the field empty': '声明应该更新的目录的名称。这可能不适用于正在更新的数据库。在这种情况下，将字段保留为空。',
  'The port that the remote system is listening on for file transfers': '远程系统监听文件传输的端口',
  'If true, will pull files in the order in which they are naturally listed; otherwise, the order in which the files will be pulled is not defined': '如果为true，将按它们自然列出的顺序拉文件；否则，将不调用文件的顺序。',
  'Use Natural Ordering': '使用自然排序',
  'Tells the client to use UTF-8 encoding when processing files and filenames. If set to true, the server must also support UTF-8 encoding.': '告诉客户端在处理文件和文件名时使用UTF-8编码。如果设置为true，服务器还必须支持UTF-8编码。',
  'If true, will pull files from arbitrarily nested subdirectories; otherwise, will not traverse subdirectories': '如果为true，则将从任意嵌套子目录中提取文件；否则，将不遍历子目录。',
  'The value specifies how many file paths to find in a given directory on the remote system when doing a file listing. This value in general should not need to be modified but when polling against a remote system with a tremendous number of files this value can be critical.  Setting this value too high can result very poor performance and setting it too low can cause the flow to be slower than normal.': '该值指定在进行文件列表时在远程系统上的给定目录中查找多少文件路径。这个值一般不需要修改，但是当对一个具有大量文件的远程系统进行轮询时，这个值可能是关键的。设置这个值太高会导致非常差的性能，并且设置它太低会导致流量比正常慢。',
  'The path on the remote system from which to pull or push files': '远程系统上从哪个路径拉动或推送文件',
  'The fully qualified hostname or IP address of the proxy server': '代理服务器的完全限定主机名或IP地址',
  'If true, files whose names begin with a dot (".") will be ignored': '如果为true，将以名称（“.”）开头的文件将被忽略。',
  'The fully qualified hostname or IP address of the remote system': '远程系统的完全限定主机名或IP地址',
  'Determines whether or not the file is deleted from the remote system after it has been successfully transferred': '确定文件在成功传输后是否从远程系统中删除。',
  'When transferring a file between the local and remote system, this value specifies how long is allowed to elapse without any data being transferred between systems': '当在本地系统和远程系统之间传输文件时，该值指定在没有系统之间传输任何数据的情况下允许访问多长时间。',
  'Provides a Java Regular Expression for filtering Filenames; if a filter is supplied, only files whose names match that Regular Expression will be fetched': '提供用于过滤文件名的Java正则表达式；如果提供了过滤器，则只获取名称与正则表达式匹配的文件。',
  'Obtain Generated Keys': '获取生成的密钥',
  'Specifies whether the Topic(s) provided are a comma separated list of names or a single regular expression': '指定提供的主题是否是逗号分隔的名称列表或单个正则表达式',
  'Log prefix appended to the log lines. It helps to distinguish the output of multiple LogMessage processors.': '附加到日志行的日志前缀。它有助于区分多个日志消息处理器的输出。',
  'The Log Level to use when logging the message': '记录消息时使用的日志级别',
  'The Kerberos principal name that Kafka runs as. This can be defined either in Kafka\'s JAAS config or in Kafka\'s config. Corresponds to Kafka\'s \'security.protocol\' property.It is ignored unless one of the SASL options of the <Security Protocol> are selected.': 'Kafka运行的Kerberos主体名称。 这可以在Kafka的JAAS配置或Kafka的配置中定义。 对应于Kafka的\'security.protocol\'属性。除非选择了<Security Protocol>的SASL选项之一，否则它将被忽略。',
  'Protocol used to communicate with brokers. Corresponds to Kafka\'s \'security.protocol\' property.': '用于与代理通信的协议。对应于 Kafka的\'security.protocol\' 属性。',
  'Specifies the SSL Context Service to use for communicating with Kafka.': '指定用于与Kafka通信的SSL上下文服务。',
  'The name of the Kafka Topic(s) to pull from. More than one can be supplied if comma separated.': 'Kafka 主题的名称。 如果以逗号分隔，可以提供多个。',
  'The Kerberos principal that will be used to connect to brokers. If not set, it is expected to set a JAAS configuration file in the JVM properties defined in the bootstrap.conf file. This principal will be set into \'sasl.jaas.config\' Kafka\'s property.': '将用于连接到代理的Kerberos主体。 如果未设置，则应在bootstrap.conf文件中定义的JVM属性中设置JAAS配置文件。 这个委托人将被设置为\'sasl.jaas.config\'Kafka的属性。',
  'The Kerberos keytab that will be used to connect to brokers. If not set, it is expected to set a JAAS configuration file in the JVM properties defined in the bootstrap.conf file. This principal will be set into \'sasl.jaas.config\' Kafka\'s property.': '将用于连接到代理的Kerberos密钥表。 如果未设置，则应在bootstrap.conf 文件中定义的JVM属性中设置JAAS配置文件。 这个委托人将被设置为\'sasl.jaas.config\'Kafka的属性。',
  'Since KafkaConsumer receives messages in batches, you have an option to output FlowFiles which contains all Kafka messages in a single batch for a given topic and partition and this property allows you to provide a string (interpreted as UTF-8) to use for demarcating apart multiple Kafka messages. This is an optional property and if not provided each Kafka message received will result in a single FlowFile which  time it is triggered. To enter special character such as \'new line\' use CTRL+Enter or Shift+Enter depending on the OS': '由于KafkaConsumer批量接收消息，因此您可以选择输出FlowFiles，其中包含给定主题和分区的单个批处理中的所有Kafka消息，此属性允许您提供用于划分分界的字符串（解释为UTF-8） 多个Kafka消息。 这是一个可选属性，如果没有提供，则每次收到的Kafka消息都将导致单个FlowFile被触发的时间。 要输入特殊字符，例如“新行”，请根据操作系统使用CTRL + Enter或Shift + Enter',
  'Specifies the maximum number of records Kafka should return in a single poll.': '指定Kafka在单个轮询中应返回的最大记录数。',
  'Specifies the maximum amount of time allowed to pass before offsets must be committed. This value impacts how often offsets will be committed.  Committing offsets less often increases throughput but also increases the window of potential data duplication in the event of a rebalance or JVM restart between commits.  This value is also related to maximum poll records and the use of a message demarcator.  When using a message demarcator we can have far more uncommitted messages than when we\'re not as there is much less for us to keep track of in memory.': '指定必须提交偏移之前允许通过的最大时间量。 此值会影响提交偏移的频率。 较少提交偏移会增加吞吐量，但如果在提交之间重新平衡或JVM重新启动，也会增加潜在数据重复的窗口。 此值还与最大轮询记录和消息分隔符的使用有关。 当使用消息分界符时，我们可以拥有比未使用时更多的未提交消息，因为我们在内存中跟踪的内容要少得多。',
  'FlowFiles that are emitted have an attribute named \'kafka.key\'. This property dictates how the value of the attribute should be encoded.': '发出的FlowFiles具有名为“kafka.key”的属性。 此属性指示应如何编码属性的值。',
  'A comma-separated list of known Kafka Brokers in the format <host>:<port>': '以逗号分隔的已知Kafka代理列表，格式为<host>:<port>',
  'The SQL statement to execute. The statement can be empty, a constant value, or built from attributes using Expression Language. If this property is specified, it will be used regardless of the content of incoming flowfiles. If this property is empty, the content of the incoming flow file is expected to contain a valid SQL statement, to be issued by the processor to the database.': '要执行的SQL语句。语句可以是空的、常量的值，也可以使用表达式语言从属性构建。如果指定了此属性，则无论传入的流文件的内容如何，都将使用该属性。如果此属性为空，则传入的流文件的内容预期包含一个有效的SQL语句，由处理器向数据库发出。',
  'Allows you to manage the condition when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted). Corresponds to Kafka\'s \'auto.offset.reset\' property.': '允许您在Kafka中没有初始偏移量或服务器上不再存在当前偏移量时管理条件（例如，因为该数据已被删除）。 对应于Kafka的\'auto.offset.reset\'属性。',
  'Allows a user to view the category': '允许用户查看类别',
  'Feed Creator': '流程创造者',
  'Allows a user to edit,export a template': '允许用户编辑，导出模板',
  'Allows a user to view the template': '允许用户查看模板',
  'Allows a user to edit,delete datasources': '允许用户编辑，删除数据源',
  'Allows a user to view the datasource': '允许用户查看数据源',
  'Allows a user to create a new feed using this category': '允许用户使用此类别创建新流程',
  'Allows a user to edit, export and delete category. Allows creating feeds under the category': '允许用户编辑，导出和删除类别。允许在类别下创建新流程',
  'es-http-url': 'ES服务URL地址',
  'Processing method': '处理方式',
  Metadata: '元数据',
  Exchange: '数据治理',
  Schedule: '计划',
  'Row Identifier Field Name': '行标识符字段名称',
  'Column Family': '列族',
  ExecuteSQL: '执行SQL',
  ConsumeKafka_0_10: 'ConsumeKafka_0_10',
  'bootstrap.servers': 'bootstrap.servers',
  topic: '主题',
  'group.id': 'group.id',
  GetFile: 'GetFile',
  GetAPI: 'GetAPI',
  URL: '路径',
  Filename: '文件名',
  'HBase Client Service': 'HBase客户服务',
  FULL_LOAD: '满载',
  INCREMENTAL: '增加',
  'Mask Credit Card': '信用卡脱敏'


}

